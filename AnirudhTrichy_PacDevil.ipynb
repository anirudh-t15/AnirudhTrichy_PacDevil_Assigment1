{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet \"stable-baselines3==2.1.0\"\n",
    "!pip install --quiet \"gymnasium[classic-control]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO\n",
    "import os\n",
    "import torch as th\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "th.manual_seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from gymnasium import spaces\n",
    "\n",
    "class OneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        assert isinstance(env.observation_space, spaces.Discrete), \"OneHotWrapper expects discrete observation space\"\n",
    "        self.n = env.observation_space.n\n",
    "        # Box of shape (n,) with float32 - suitable for SB3 MLP policy\n",
    "        self.observation_space = spaces.Box(low=0.0, high=1.0, shape=(self.n,), dtype=np.float32)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        # obs will be an integer state\n",
    "        arr = np.zeros(self.n, dtype=np.float32)\n",
    "        arr[int(obs)] = 1.0\n",
    "        return arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def make_env(seed=None):\n",
    "    env = gym.make(\"Taxi-v3\")\n",
    "    env = OneHotWrapper(env)\n",
    "    if seed is not None:\n",
    "        env.reset(seed=seed)\n",
    "    return env\n",
    "\n",
    "\n",
    "env = make_env(seed=SEED)\n",
    "obs, info = env.reset()\n",
    "print(\"Initial observation (one-hot) shape:\", obs.shape, \"  sum:\", obs.sum())  # sum == 1.0\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "policy_kwargs = dict(net_arch=[128, 128])  # modest MLP\n",
    "\n",
    "def train_ppo(env_fn, total_timesteps=200_000, learning_rate=3e-4, seed=SEED, model_name=\"ppo_model\"):\n",
    "    env = env_fn(seed=seed)\n",
    "    # force CPU device to avoid CUDA complications\n",
    "    model = PPO(\"MlpPolicy\", env, learning_rate=learning_rate, verbose=1,\n",
    "                policy_kwargs=policy_kwargs, seed=seed, device=\"cpu\")\n",
    "    model.learn(total_timesteps=total_timesteps)\n",
    "    # save model\n",
    "    model_path = f\"{model_name}_lr{learning_rate:.0e}.zip\"\n",
    "    model.save(model_path)\n",
    "    env.close()\n",
    "    return model, model_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test_agent(model, env_fn, episodes=10, render=False):\n",
    "    env = env_fn()\n",
    "    rewards = []\n",
    "    for ep in range(episodes):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            action = int(action)                      # important: Taxi expects int\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            if render:\n",
    "                # text rendering (works in classic-control)\n",
    "                try:\n",
    "                    print(env.unwrapped.desc)  # may or may not be helpful; optional\n",
    "                except Exception:\n",
    "                    pass\n",
    "        rewards.append(total_reward)\n",
    "        print(f\"Episode {ep+1} Total Reward: {total_reward}  (steps: {steps})\")\n",
    "    env.close()\n",
    "    return rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "default_timesteps = 200_000\n",
    "print(\"Training PPO with default LR = 3e-4 ...\")\n",
    "model_default, path_default = train_ppo(make_env, total_timesteps=default_timesteps, learning_rate=3e-4, model_name=\"ppo_default\")\n",
    "print(\"Saved default model to:\", path_default)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Testing Default PPO ---\")\n",
    "default_rewards = test_agent(model_default, make_env, episodes=20)\n",
    "print(\"Average reward (default):\", np.mean(default_rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "aggressive_lr = 1e-3\n",
    "aggressive_timesteps = 200_000\n",
    "print(f\"Training PPO with aggressive LR = {aggressive_lr} ...\")\n",
    "model_aggr, path_aggr = train_ppo(make_env, total_timesteps=aggressive_timesteps, learning_rate=aggressive_lr, model_name=\"ppo_aggressive\")\n",
    "print(\"Saved aggressive model to:\", path_aggr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Testing Aggressive-LR PPO ---\")\n",
    "aggr_rewards = test_agent(model_aggr, make_env, episodes=20)\n",
    "print(\"Average reward (aggressive):\", np.mean(aggr_rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(default_rewards, label=f\"default lr=3e-4 (avg={np.mean(default_rewards):.2f})', marker='o')\n",
    "plt.plot(aggr_rewards, label=f'aggressive lr={aggressive_lr} (avg={np.mean(aggr_rewards):.2f})', marker='x')\n",
    "plt.xlabel(\"Test episode\")\n",
    "plt.ylabel(\"Total reward\")\n",
    "plt.title(\"Per-episode Total Reward (comparison)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(\"Default avg:\", np.mean(default_rewards), \"Aggressive avg:\", np.mean(aggr_rewards))\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
